{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Copyright-protected material, all rights reserved. (c) University of Vienna.\n",
    "_Copyright Notice of the corresponding course at Moodle applies. <br> Only to be used in the MRE course._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRE Assignment 3 - Digital Video Processing\n",
    "In this assignment, you will use OpenCV and FFmpeg to implement very basic video editing functions. These tasks include:\n",
    "\n",
    "1. Create a slide show (as a video) from images, and optionally create the slideshow as greyscale video.\n",
    "2. Extract the audio track from a video file.\n",
    "3. Replace the audio track in a video file.\n",
    "4. Combine two or more videos into one video file.\n",
    "5. Blend an image (fade-in/fade-out) with a video.\n",
    "6. Blend two videos into one video (video collage).\n",
    "\n",
    "In this notebook, you will implement your solution. This notebook will be imported into the \"*_def.ipynb\" notebook.\n",
    "\n",
    "Of course you can include code for testing your implementation in this implementation notebook, but code for testing and output generated for testing is not going to be assessed.\n",
    "\n",
    "Of course, your code for the solutions in this notebook will be inspected and is subject to grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For general installation instructions, please refer to the ressources given for all the assignments [in Moodle](https://moodle.univie.ac.at/course/view.php?id=164140#section-12).\n",
    "\n",
    "If the cell below executes without error, you can start the assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Imports --------\n",
    "# Please do not change the contents of this cell!\n",
    "\n",
    "# Imports required by us.\n",
    "import cv2              # opencv-python\n",
    "import ffmpeg           # ffmpeg-python\n",
    "import subprocess   # for calling local executables such as ffmpeg.exe\n",
    "import pandas as pd  # pandas\n",
    "from fractions import Fraction as frac # simplifying fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below, place your own imports, global variables, (helper) functions and classes. Feel free to add cells here as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your own imports here.\n",
    "\n",
    "#This imports were missing from the top cell\n",
    "import IPython\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any helper functions, global variables and classes here.\n",
    "\n",
    "# For example:the function you need to play back a mp4-video file.\n",
    "# You may use this function to display the videos in your definition file during the assessment for demoing the solutions.\n",
    "def VideoPlayer(videoFile: str) -> None:\n",
    "    \n",
    "    IPython.display.display(\n",
    "        HTML(\"\"\"\n",
    "            <video alt=\"test\" controls>\n",
    "            <source src={} type=\"video/mp4\">\n",
    "            </video>\n",
    "        \"\"\".format(videoFile))\n",
    "    )\n",
    "    \n",
    "    return\n",
    "\n",
    "def outputDir(dir: str):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <video alt=\"test\" controls>\n",
       "            <source src=results/t6/output.mp4 type=\"video/mp4\">\n",
       "            </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#VideoPlayer(\"results/t6/output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3.1: Create a slide show (Video) from multiple images and convert it to greyscale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here.\n",
    "\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "def CreateVideoFromImages(\n",
    "        inImgLibFolder: str,   \\\n",
    "        imageFormat: str,       \\\n",
    "        durationInSec: float,    \\\n",
    "        convertToGreyScale: bool, \\\n",
    "        outFolder: str,            \\\n",
    "        outVideo: str) -> None:\n",
    "    \n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "\n",
    "    #                             Image folder               REGEX MOTOR          DURATION PER IMAGE\n",
    "    inp = ffmpeg.input(inImgLibFolder+'/*.'+imageFormat, pattern_type='glob', framerate=1/durationInSec)\n",
    "\n",
    "    if convertToGreyScale:\n",
    "        inp = inp.filter(\"format\", \"gray\")\n",
    "    \n",
    "    inp.output(outFolder+'/'+outVideo+'.mp4')\\\n",
    "        .run(overwrite_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test your function here.\n",
    "#CreateVideoFromImages(\"resources/images\", \"JPG\", 2, True, \"results/t1\", \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3.2: Extract the Audio Track from a Video File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here.\n",
    "\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "#Adapted from https://www.reddit.com/r/ffmpeg/comments/y4xtzs/using_ffmpeg_to_split_the_audio_and_video_into/\n",
    "def SplitAudioVideoTracks(\n",
    "        inVideo: str,     \\\n",
    "        outFolder: str,    \\\n",
    "        outVideoTrack: str, \\\n",
    "        outAudioTrack: str) -> None:\n",
    "    \n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "    \n",
    "    inp = ffmpeg.input(inVideo)\n",
    "    \n",
    "    if outVideoTrack != None:\n",
    "        #                 OUTPUT DIR                       VIDEO CODED    DISABLE AUDIO\n",
    "        inp.output(outFolder+\"/\"+outVideoTrack+\".mp4\", **{\"c:v\": \"copy\"}, an=None)\\\n",
    "        .run(overwrite_output=True)\n",
    "    \n",
    "    if outAudioTrack != None:\n",
    "        #                 OUTPUT DIR                       AUDIO CODED    DISABLE VIDEO\n",
    "        inp.output(outFolder+\"/\"+outAudioTrack+\".mp4\", **{\"c:a\": \"copy\"}, vn=None)\\\n",
    "        .run(overwrite_output=True) #ffmpeg -i infile.mp4 -an -c:v copy videoout.mp4 -vn -c:a copy audioout.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function here.\n",
    "#SplitAudioVideoTracks(\"resources/video/sample1.mp4\", \"results/t2\", \"videoOnly\", \"audioOnly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3.3: Replace the Audio Track in a Video File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here.\n",
    "\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "\n",
    "def AddOrReplaceAudio(\n",
    "        inVideo: str, \\\n",
    "        inAudio: str,  \\\n",
    "        outFolder:str,  \\\n",
    "        outVideo: str) -> None:\n",
    "    \n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "    \n",
    "    if inAudio == None: #If no audio is provided, the video is muted\n",
    "        SplitAudioVideoTracks(inVideo, outFolder, outVideo, None)\n",
    "        return\n",
    "    \n",
    "    #JOIN STREAMS   VIDEO STREAM              AUDIO STREAM      AUDIO TRUE  VIDEO TRUE\n",
    "    ffmpeg.concat(ffmpeg.input(inVideo), ffmpeg.input(inAudio),     a=1    ,    v=1)\\\n",
    "    .output(outFolder+\"/\"+outVideo+\".mp4\", t=float(ffmpeg.probe(inVideo)[\"streams\"][0][\"duration\"])).run(overwrite_output=True)\n",
    "    #                 OUTPUT FILE                            TRIM TO VIDEO DURATION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AddOrReplaceAudio(\"resources/video/sample1.mp4\", \"resources/audio/Amazon.mp3\", \"results/t3\", \"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3.4: Combine Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here.\n",
    "VIDEO_FORMATS = [\"mp4\", \"avi\"]\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "def CombineVideos(\n",
    "        inVideoLibFolder: str, \\\n",
    "        outFolder: str,         \\\n",
    "        outVideo: str) -> None:\n",
    "    \n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "    \n",
    "    global VIDEO_FORMATS\n",
    "    VIDEOS = [glob.glob(inVideoLibFolder+\"/*.\"+n) for n in VIDEO_FORMATS]\n",
    "\n",
    "    #Output video properties\n",
    "    height = 0\n",
    "    width = 0\n",
    "    fps = 0\n",
    "    \n",
    "    #                GRAB ALL MP4 FILES                 GRAB ALL AVI FILES\n",
    "    for v in VIDEOS:\n",
    "        for video in v:\n",
    "            md = VideoMetadataExtractor(*video.rsplit(\"/\", 1))\n",
    "            height = max(height, md[\"vHeight\"])\n",
    "            width = max(width, md[\"vWidth\"])\n",
    "            fps = max(fps, md[\"vFPS\"])\n",
    "\n",
    "    #List of all video inputs in ffmpeg \n",
    "    inputs = []\n",
    "\n",
    "    for v in VIDEOS:\n",
    "        for video in v:\n",
    "            inp = ffmpeg.input(video)\n",
    "            inputs += [\n",
    "                inp.video\n",
    "                    .filter('scale', f\"{width}x{height}\"),\n",
    "                inp.audio] #Divide the video from the audio for the concat\n",
    "    \n",
    "    #https://www.reddit.com/r/learnpython/comments/rbr32l/combine_list_of_video_into_single_video/\n",
    "    node = ffmpeg.concat(*inputs, v=1, a=1).node\n",
    "    ffmpeg.output(node[0], node[1], outFolder+\"/\"+outVideo+\".mp4\", r=fps)\\\n",
    "    .run(overwrite_output=True)\n",
    "\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "def VideoMetadataExtractor(\n",
    "        videoFolder: str, \\\n",
    "        video: str =None) -> dict:\n",
    "    \n",
    "    global VIDEO_FORMATS\n",
    "    \n",
    "    #If no video is provided , a dataframe is generated\n",
    "    if(video == None):\n",
    "        files = []\n",
    "    \n",
    "        for format_videos in [glob.glob(videoFolder+\"/*.\"+n) for n in VIDEO_FORMATS]: \n",
    "            for vid in format_videos: \n",
    "                files.append(vid)\n",
    "\n",
    "        return pd.concat([pd.DataFrame(data=VideoMetadataExtractor(videoFolder, data.split(\"/\")[-1]), index=[i]) for i, data in enumerate(files)])\n",
    "    \n",
    "    #If a video is provided, the information is returned in a dictionary format both to fill the dataframe or grab directly the information\n",
    "    streams = ffmpeg.probe(videoFolder+\"/\"+video)['streams']\n",
    "    videoStream = streams[0]\n",
    "    audioStream = streams[1]\n",
    "    \n",
    "    return {\n",
    "        \"vCodec\"      : videoStream[\"codec_name\"],\n",
    "        \"vCodecID\"    : int(videoStream[\"codec_tag\"], 16),\n",
    "        \"vDur\"        : videoStream[\"duration\"],\n",
    "        \"vFPS\"        : int(round(eval(videoStream[\"avg_frame_rate\"]), 0)),\n",
    "        \"vHeight\"     : videoStream[\"height\"],\n",
    "        \"vWidth\"      : videoStream[\"width\"],\n",
    "        \n",
    "        \"aCodec\"      : audioStream[\"codec_name\"],\n",
    "        \"aCodecID\"    : int(audioStream[\"codec_tag\"], 16),\n",
    "        \"aChannels\"   : audioStream[\"channels\"],\n",
    "        \"aSampleRate\" : audioStream[\"sample_rate\"],\n",
    "        \"aBitRate\"    : audioStream[\"bit_rate\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vCodec</th>\n",
       "      <th>vCodecID</th>\n",
       "      <th>vDur</th>\n",
       "      <th>vFPS</th>\n",
       "      <th>vHeight</th>\n",
       "      <th>vWidth</th>\n",
       "      <th>aCodec</th>\n",
       "      <th>aCodecID</th>\n",
       "      <th>aChannels</th>\n",
       "      <th>aSampleRate</th>\n",
       "      <th>aBitRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h264</td>\n",
       "      <td>828601953</td>\n",
       "      <td>42.375667</td>\n",
       "      <td>30</td>\n",
       "      <td>240</td>\n",
       "      <td>352</td>\n",
       "      <td>aac</td>\n",
       "      <td>1630826605</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>127761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h264</td>\n",
       "      <td>828601953</td>\n",
       "      <td>7.107100</td>\n",
       "      <td>30</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>aac</td>\n",
       "      <td>1630826605</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>128289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h264</td>\n",
       "      <td>875967048</td>\n",
       "      <td>7.173833</td>\n",
       "      <td>24</td>\n",
       "      <td>486</td>\n",
       "      <td>720</td>\n",
       "      <td>aac</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>128000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vCodec   vCodecID       vDur  vFPS  vHeight  vWidth aCodec    aCodecID  \\\n",
       "0   h264  828601953  42.375667    30      240     352    aac  1630826605   \n",
       "1   h264  828601953   7.107100    30      480     640    aac  1630826605   \n",
       "2   h264  875967048   7.173833    24      486     720    aac         255   \n",
       "\n",
       "   aChannels aSampleRate aBitRate  \n",
       "0          2       44100   127761  \n",
       "1          2       44100   128289  \n",
       "2          2       44100   128000  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function here.\n",
    "#VideoMetadataExtractor(\"resources/video\")\n",
    "#CombineVideos(\"resources/video\", \"results/t4\", \"combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3.5: Blend an Image in a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here!\n",
    "\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "def AddFadingImage(inVideo: str, inImg: str, time: float, outFolder: str, outVideo: str) -> None: \n",
    "\n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "    \n",
    "    t = float(ffmpeg.probe(inVideo)[\"streams\"][0][\"duration\"])#Total time\n",
    "\n",
    "    duration = max(0.01, min((t - time)/2, 3)) #Fading time (the difference between the visible time and the total time bounded to [0.01, 3] seconds)\n",
    "\n",
    "    img = ffmpeg.input(inImg, t=42, loop=1)\\\n",
    "                .filter(\"scale\", \"-1\", \"100\")\\\n",
    "                .filter(\"fade\", type=\"in\", st=0, d=duration, alpha=1)\\\n",
    "                .filter(\"fade\", t=\"out\", st=min(duration+time, t - duration), d=duration, alpha=1)#The start time (st) is set to the fade in + duration or the total time - the fade duration if the input duration is too long\n",
    "    \n",
    "    \n",
    "    ffmpeg.filter([ffmpeg.input(inVideo), img], 'overlay', 10, 10)\\\n",
    "        .output(outFolder + \"/\" + outVideo, map= \"0:a\")\\\n",
    "        .run(overwrite_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "ffmpeg error (see stderr output for detail)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test your function here.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mAddFadingImage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresources/video/sample1.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresources/images/elephant14m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m430\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/t5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mAddFadingImage\u001b[1;34m(inVideo, inImg, time, outFolder, outVideo)\u001b[0m\n\u001b[0;32m      8\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;28mmin\u001b[39m((t \u001b[38;5;241m-\u001b[39m time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;66;03m#Fading time (the difference between the visible time and the total time bounded to [0.01, 3] seconds)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m ffmpeg\u001b[38;5;241m.\u001b[39minput(inImg, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\\\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfade\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m, st\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, d\u001b[38;5;241m=\u001b[39mduration, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\\\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfade\u001b[39m\u001b[38;5;124m\"\u001b[39m, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m, st\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(duration\u001b[38;5;241m+\u001b[39mtime, t \u001b[38;5;241m-\u001b[39m duration), d\u001b[38;5;241m=\u001b[39mduration, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#The start time (st) is set to the fade in + duration or the total time - the fade duration if the input duration is too long\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m(\u001b[49m\u001b[43minVideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverlay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutFolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutVideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0:a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverwrite_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\virtual_envs\\env3\\lib\\site-packages\\ffmpeg\\_run.py:325\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(stream_spec, cmd, capture_stdout, capture_stderr, input, quiet, overwrite_output)\u001b[0m\n\u001b[0;32m    323\u001b[0m retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffmpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, out, err)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, err\n",
      "\u001b[1;31mError\u001b[0m: ffmpeg error (see stderr output for detail)"
     ]
    }
   ],
   "source": [
    "# Test your function here.\n",
    "AddFadingImage(\"resources/video/sample1.mp4\", \"resources/images/elephant14m.png\", 430, \"results/t5\", \"result.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.6: Create a Video Collage - blend two videos into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here.\n",
    "class Layout(Enum):\n",
    "    HORIZONTAL = True\n",
    "    VERTICAL = False\n",
    "# Replace the parameters and return type of the following function according to the task specification. \n",
    "def VideoClipMixer(\n",
    "        inVideo1: str,       \\\n",
    "        inVideo2: str,        \\\n",
    "        layout: Layout, \\\n",
    "        outFolder: str,         \\\n",
    "        outVideo: str) -> None: \n",
    "    \n",
    "    outputDir(outFolder)\n",
    "    \n",
    "    i = int(layout.value)\n",
    "    ni = 1-i\n",
    "    d = [\"vstack\", \"hstack\"][i]\n",
    "    \n",
    "    p1 = ffmpeg.probe(inVideo1)[\"streams\"][0]\n",
    "    p2 = ffmpeg.probe(inVideo2)[\"streams\"][0]\n",
    "    \n",
    "    p1 = [int(p1[\"width\"]), int(p1[\"height\"])]\n",
    "    p2 = [int(p2[\"width\"]), int(p2[\"height\"])]\n",
    "    \n",
    "    dimensions = [str(max(p1[0], p2[0])), str(max(p1[1], p2[1]))]\n",
    "    \n",
    "    dimensions[ni] = \"a\"\n",
    "    \n",
    "    p1[ni] *= int(dimensions[i]) / p1[i]\n",
    "    p2[ni] *= int(dimensions[i]) / p2[i]\n",
    "    \n",
    "    \n",
    "    in0 = ffmpeg.input(inVideo1).filter(\"scale\",\"x\".join(dimensions).replace(\"a\", str(int(p1[ni]))))\n",
    "    in1 = ffmpeg.input(inVideo2).filter(\"scale\",\"x\".join(dimensions).replace(\"a\", str(int(p1[ni]))))\n",
    "    \n",
    "    out = ffmpeg.filter((in0, in1), d)\n",
    "    ffmpeg.output(out, outFolder+\"/\"+outVideo+\".mp4\", r=60)\\\n",
    "        .run(overwrite_output=True)#ffmpeg -i left.mp4 -i right.mp4 -filter_complex hstack output.mp4\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'resources/video/sample2.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf57.83.100\n",
      "  Duration: 00:00:07.69, start: 0.033000, bitrate: 4940 kb/s\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x480, 5195 kb/s, 30.11 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Input #1, avi, from 'resources/video/sample3.avi':\n",
      "  Metadata:\n",
      "    software        : Lavf57.83.100\n",
      "  Duration: 00:00:07.87, start: 0.000000, bitrate: 5320 kb/s\n",
      "  Stream #1:0: Video: h264 (High) (H264 / 0x34363248), yuv420p(progressive), 720x486, 5712 kb/s, 23.98 fps, 23.98 tbr, 23.98 tbn, 47.95 tbc\n",
      "  Stream #1:1: Audio: aac (LC) ([255][0][0][0] / 0x00FF), 44100 Hz, stereo, fltp, 128 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> scale\n",
      "  Stream #1:0 (h264) -> scale\n",
      "  vstack -> Stream #0:0 (libx264)\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x5639ef625080] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x5639ef625080] profile High, level 3.2, 4:2:0, 8-bit\n",
      "[libx264 @ 0x5639ef625080] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'results/t6/output.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(progressive), 720x1080, q=2-31, 60 fps, 15360 tbn (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=  429 fps= 90 q=-1.0 Lsize=    1901kB time=00:00:07.10 bitrate=2193.8kbits/s dup=120 drop=33 speed=1.49x    \n",
      "video:1896kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.309319%\n",
      "[libx264 @ 0x5639ef625080] frame I:2     Avg QP:19.17  size: 68675\n",
      "[libx264 @ 0x5639ef625080] frame P:108   Avg QP:25.02  size: 14128\n",
      "[libx264 @ 0x5639ef625080] frame B:319   Avg QP:23.21  size:   869\n",
      "[libx264 @ 0x5639ef625080] consecutive B-frames:  0.5%  0.5%  2.1% 97.0%\n",
      "[libx264 @ 0x5639ef625080] mb I  I16..4: 41.0% 33.7% 25.3%\n",
      "[libx264 @ 0x5639ef625080] mb P  I16..4:  0.6%  2.6%  0.8%  P16..4: 33.0% 15.7%  9.6%  0.0%  0.0%    skip:37.7%\n",
      "[libx264 @ 0x5639ef625080] mb B  I16..4:  0.3%  0.1%  0.0%  B16..8: 14.7%  0.8%  0.2%  direct: 0.5%  skip:83.5%  L0:44.8% L1:51.0% BI: 4.2%\n",
      "[libx264 @ 0x5639ef625080] 8x8 transform intra:48.1% inter:63.0%\n",
      "[libx264 @ 0x5639ef625080] coded y,uvDC,uvAC intra: 55.0% 38.0% 12.9% inter: 7.1% 4.0% 0.2%\n",
      "[libx264 @ 0x5639ef625080] i16 v,h,dc,p: 79% 17%  2%  3%\n",
      "[libx264 @ 0x5639ef625080] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 15% 27% 17%  4%  6%  7%  8%  6% 10%\n",
      "[libx264 @ 0x5639ef625080] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 18% 28% 13%  5%  7%  8%  8%  6%  7%\n",
      "[libx264 @ 0x5639ef625080] i8c dc,h,v,p: 67% 18% 12%  4%\n",
      "[libx264 @ 0x5639ef625080] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x5639ef625080] ref P L0: 56.6% 23.7% 12.2%  7.5%\n",
      "[libx264 @ 0x5639ef625080] ref B L0: 87.2%  8.6%  4.2%\n",
      "[libx264 @ 0x5639ef625080] ref B L1: 97.9%  2.1%\n",
      "[libx264 @ 0x5639ef625080] kb/s:2171.02\n"
     ]
    }
   ],
   "source": [
    "# Test your function here.\n",
    "VideoClipMixer(\"resources/video/sample2.mp4\", \"resources/video/sample3.avi\", Layout.VERTICAL, \"results/t6\", \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
