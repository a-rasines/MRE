{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Copyright-protected material, all rights reserved. (c) University of Vienna.\n",
    "_Copyright Notice of the corresponding course at Moodle applies. <br> Only to be used in the MRE course._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRE Assignment 3 - Digital Video Processing\n",
    "In this assignment, you will use OpenCV and FFmpeg to implement very basic video editing functions. These tasks include:\n",
    "\n",
    "1. Create a slide show (as a video) from images, and optionally create the slideshow as greyscale video.\n",
    "2. Extract the audio track from a video file.\n",
    "3. Replace the audio track in a video file.\n",
    "4. Combine two or more videos into one video file.\n",
    "5. Blend an image (fade-in/fade-out) with a video.\n",
    "6. Blend two videos into one video (video collage).\n",
    "\n",
    "In this notebook, you will implement your solution. This notebook will be imported into the \"*_def.ipynb\" notebook.\n",
    "\n",
    "Of course you can include code for testing your implementation in this implementation notebook, but code for testing and output generated for testing is not going to be assessed.\n",
    "\n",
    "Of course, your code for the solutions in this notebook will be inspected and is subject to grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For general installation instructions, please refer to the ressources given for all the assignments [in Moodle](https://moodle.univie.ac.at/course/view.php?id=164140#section-12).\n",
    "\n",
    "If the cell below executes without error, you can start the assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Imports --------\n",
    "# Please do not change the contents of this cell!\n",
    "\n",
    "# Imports required by us.\n",
    "import cv2              # opencv-python\n",
    "import ffmpeg           # ffmpeg-python\n",
    "import subprocess   # for calling local executables such as ffmpeg.exe\n",
    "import pandas as pd  # pandas\n",
    "from fractions import Fraction as frac # simplifying fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below, place your own imports, global variables, (helper) functions and classes. Feel free to add cells here as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your own imports here.\n",
    "\n",
    "#This imports were missing from the top cell\n",
    "import IPython\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any helper functions, global variables and classes here.\n",
    "\n",
    "# For example:the function you need to play back a mp4-video file.\n",
    "# You may use this function to display the videos in your definition file during the assessment for demoing the solutions.\n",
    "def VideoPlayer(videoFile: str) -> None:\n",
    "    \n",
    "    IPython.display.display(\n",
    "        HTML(\"\"\"\n",
    "            <video alt=\"test\" controls>\n",
    "            <source src={} type=\"video/mp4\">\n",
    "            </video>\n",
    "        \"\"\".format(videoFile))\n",
    "    )\n",
    "    \n",
    "    return\n",
    "\n",
    "def outputDir(dir: str):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <video alt=\"test\" controls>\n",
       "            <source src=results/t6/output.mp4 type=\"video/mp4\">\n",
       "            </video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#VideoPlayer(\"results/t6/output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3.1: Create a slide show (Video) from multiple images and convert it to greyscale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here.\n",
    "\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "def CreateVideoFromImages(\n",
    "        inImgLibFolder: str,   \\\n",
    "        imageFormat: str,       \\\n",
    "        durationInSec: float,    \\\n",
    "        convertToGreyScale: bool, \\\n",
    "        outFolder: str,            \\\n",
    "        outVideo: str) -> None:\n",
    "    \n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "\n",
    "    #                             Image folder               REGEX MOTOR          DURATION PER IMAGE\n",
    "    inp = ffmpeg.input(inImgLibFolder+'/*.'+imageFormat, pattern_type='glob', framerate=1/durationInSec)\n",
    "\n",
    "    if convertToGreyScale:\n",
    "        inp = inp.filter(\"format\", \"gray\")\n",
    "    \n",
    "    inp.output(outFolder+'/'+outVideo+'.mp4')\\\n",
    "        .run(overwrite_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test your function here.\n",
    "#CreateVideoFromImages(\"resources/images\", \"JPG\", 2, True, \"results/t1\", \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3.2: Extract the Audio Track from a Video File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here.\n",
    "\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "#Adapted from https://www.reddit.com/r/ffmpeg/comments/y4xtzs/using_ffmpeg_to_split_the_audio_and_video_into/\n",
    "def SplitAudioVideoTracks(\n",
    "        inVideo: str,     \\\n",
    "        outFolder: str,    \\\n",
    "        outVideoTrack: str, \\\n",
    "        outAudioTrack: str) -> None:\n",
    "    \n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "    \n",
    "    inp = ffmpeg.input(inVideo)\n",
    "    \n",
    "    if outVideoTrack != None:\n",
    "        #                 OUTPUT DIR                       VIDEO CODED    DISABLE AUDIO\n",
    "        inp.output(outFolder+\"/\"+outVideoTrack+\".mp4\", **{\"c:v\": \"copy\"}, an=None)\\\n",
    "        .run(overwrite_output=True)\n",
    "    \n",
    "    if outAudioTrack != None:\n",
    "        #                 OUTPUT DIR                       AUDIO CODED    DISABLE VIDEO\n",
    "        inp.output(outFolder+\"/\"+outAudioTrack+\".mp4\", **{\"c:a\": \"copy\"}, vn=None)\\\n",
    "        .run(overwrite_output=True) #ffmpeg -i infile.mp4 -an -c:v copy videoout.mp4 -vn -c:a copy audioout.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function here.\n",
    "#SplitAudioVideoTracks(\"resources/video/sample1.mp4\", \"results/t2\", \"videoOnly\", \"audioOnly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3.3: Replace the Audio Track in a Video File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here.\n",
    "\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "\n",
    "def AddOrReplaceAudio(\n",
    "        inVideo: str, \\\n",
    "        inAudio: str,  \\\n",
    "        outFolder:str,  \\\n",
    "        outVideo: str) -> None:\n",
    "    \n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "    \n",
    "    if inAudio == None: #If no audio is provided, the video is muted\n",
    "        SplitAudioVideoTracks(inVideo, outFolder, outVideo, None)\n",
    "        return\n",
    "    \n",
    "    #JOIN STREAMS   VIDEO STREAM              AUDIO STREAM      AUDIO TRUE  VIDEO TRUE\n",
    "    ffmpeg.concat(ffmpeg.input(inVideo), ffmpeg.input(inAudio),     a=1    ,    v=1)\\\n",
    "    .output(outFolder+\"/\"+outVideo+\".mp4\", t=float(ffmpeg.probe(inVideo)[\"streams\"][0][\"duration\"])).run(overwrite_output=True)\n",
    "    #                 OUTPUT FILE                            TRIM TO VIDEO DURATION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AddOrReplaceAudio(\"resources/video/sample1.mp4\", \"resources/audio/Amazon.mp3\", \"results/t3\", \"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3.4: Combine Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here.\n",
    "VIDEO_FORMATS = [\"mp4\", \"avi\"]\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "def CombineVideos(\n",
    "        inVideoLibFolder: str, \\\n",
    "        outFolder: str,         \\\n",
    "        outVideo: str) -> None:\n",
    "    \n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "    \n",
    "    global VIDEO_FORMATS\n",
    "    VIDEOS = [glob.glob(inVideoLibFolder+\"/*.\"+n) for n in VIDEO_FORMATS]\n",
    "\n",
    "    #Output video properties\n",
    "    height = 0\n",
    "    width = 0\n",
    "    fps = 0\n",
    "    \n",
    "    #                GRAB ALL MP4 FILES                 GRAB ALL AVI FILES\n",
    "    for v in VIDEOS:\n",
    "        for video in v:\n",
    "            md = VideoMetadataExtractor(*video.rsplit(\"/\", 1))\n",
    "            height = max(height, md[\"vHeight\"])\n",
    "            width = max(width, md[\"vWidth\"])\n",
    "            fps = max(fps, md[\"vFPS\"])\n",
    "\n",
    "    #List of all video inputs in ffmpeg \n",
    "    inputs = []\n",
    "\n",
    "    for v in VIDEOS:\n",
    "        for video in v:\n",
    "            inp = ffmpeg.input(video)\n",
    "            inputs += [\n",
    "                inp.video\n",
    "                    .filter('scale', f\"{width}x{height}\"),\n",
    "                inp.audio] #Divide the video from the audio for the concat\n",
    "    \n",
    "    #https://www.reddit.com/r/learnpython/comments/rbr32l/combine_list_of_video_into_single_video/\n",
    "    node = ffmpeg.concat(*inputs, v=1, a=1).node\n",
    "    ffmpeg.output(node[0], node[1], outFolder+\"/\"+outVideo+\".mp4\", r=fps)\\\n",
    "    .run(overwrite_output=True)\n",
    "\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "def VideoMetadataExtractor(\n",
    "        videoFolder: str, \\\n",
    "        video: str =None) -> dict:\n",
    "    \n",
    "    global VIDEO_FORMATS\n",
    "    \n",
    "    #If no video is provided , a dataframe is generated\n",
    "    if(video == None):\n",
    "        files = []\n",
    "    \n",
    "        for format_videos in [glob.glob(videoFolder+\"/*.\"+n) for n in VIDEO_FORMATS]: \n",
    "            for vid in format_videos: \n",
    "                files.append(vid)\n",
    "\n",
    "        return pd.concat([pd.DataFrame(data=VideoMetadataExtractor(videoFolder, data.split(\"/\")[-1]), index=[i]) for i, data in enumerate(files)])\n",
    "    \n",
    "    #If a video is provided, the information is returned in a dictionary format both to fill the dataframe or grab directly the information\n",
    "    streams = ffmpeg.probe(videoFolder+\"/\"+video)['streams']\n",
    "    videoStream = streams[0]\n",
    "    audioStream = streams[1]\n",
    "    \n",
    "    return {\n",
    "        \"vCodec\"      : videoStream[\"codec_name\"],\n",
    "        \"vCodecID\"    : int(videoStream[\"codec_tag\"], 16),\n",
    "        \"vDur\"        : videoStream[\"duration\"],\n",
    "        \"vFPS\"        : int(round(eval(videoStream[\"avg_frame_rate\"]), 0)),\n",
    "        \"vHeight\"     : videoStream[\"height\"],\n",
    "        \"vWidth\"      : videoStream[\"width\"],\n",
    "        \n",
    "        \"aCodec\"      : audioStream[\"codec_name\"],\n",
    "        \"aCodecID\"    : int(audioStream[\"codec_tag\"], 16),\n",
    "        \"aChannels\"   : audioStream[\"channels\"],\n",
    "        \"aSampleRate\" : audioStream[\"sample_rate\"],\n",
    "        \"aBitRate\"    : audioStream[\"bit_rate\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vCodec</th>\n",
       "      <th>vCodecID</th>\n",
       "      <th>vDur</th>\n",
       "      <th>vFPS</th>\n",
       "      <th>vHeight</th>\n",
       "      <th>vWidth</th>\n",
       "      <th>aCodec</th>\n",
       "      <th>aCodecID</th>\n",
       "      <th>aChannels</th>\n",
       "      <th>aSampleRate</th>\n",
       "      <th>aBitRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h264</td>\n",
       "      <td>828601953</td>\n",
       "      <td>42.375667</td>\n",
       "      <td>30</td>\n",
       "      <td>240</td>\n",
       "      <td>352</td>\n",
       "      <td>aac</td>\n",
       "      <td>1630826605</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>127761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h264</td>\n",
       "      <td>828601953</td>\n",
       "      <td>7.107100</td>\n",
       "      <td>30</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>aac</td>\n",
       "      <td>1630826605</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>128289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h264</td>\n",
       "      <td>875967048</td>\n",
       "      <td>7.173833</td>\n",
       "      <td>24</td>\n",
       "      <td>486</td>\n",
       "      <td>720</td>\n",
       "      <td>aac</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>128000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vCodec   vCodecID       vDur  vFPS  vHeight  vWidth aCodec    aCodecID  \\\n",
       "0   h264  828601953  42.375667    30      240     352    aac  1630826605   \n",
       "1   h264  828601953   7.107100    30      480     640    aac  1630826605   \n",
       "2   h264  875967048   7.173833    24      486     720    aac         255   \n",
       "\n",
       "   aChannels aSampleRate aBitRate  \n",
       "0          2       44100   127761  \n",
       "1          2       44100   128289  \n",
       "2          2       44100   128000  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function here.\n",
    "#VideoMetadataExtractor(\"resources/video\")\n",
    "#CombineVideos(\"resources/video\", \"results/t4\", \"combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Task 3.5: Blend an Image in a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here!\n",
    "\n",
    "# Replace the parameters and return type of the following function according to the task specification.\n",
    "def AddFadingImage(inVideo: str, inImg: str, time: float, outFolder: str, outVideo: str) -> None: \n",
    "\n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "\n",
    "    t = float(ffmpeg.probe(inVideo)[\"streams\"][0][\"duration\"])#Total time\n",
    "\n",
    "    duration = max(0.01, min((t - time)/2, 3)) #Fading time (the difference between the visible time and the total time bounded to [0.01, 3] seconds)\n",
    "\n",
    "    img = ffmpeg.input(inImg, t=42, loop=1)\\\n",
    "                .filter(\"scale\", \"-1\", \"100\")\\\n",
    "                .filter(\"fade\", type=\"in\", st=0, d=duration, alpha=1)\\\n",
    "                .filter(\"fade\", t=\"out\", st=min(duration+time, t - duration), d=duration, alpha=1)#The start time (st) is set to the fade in + duration or the total time - the fade duration if the input duration is too long\n",
    "    \n",
    "    \n",
    "    ffmpeg.filter([ffmpeg.input(inVideo), img], 'overlay', 10, 10)\\\n",
    "        .output(outFolder + \"/\" + outVideo, map= \"0:a\")\\\n",
    "        .run(overwrite_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "ffmpeg error (see stderr output for detail)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test your function here.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mAddFadingImage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresources/video/sample1.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresources/images/elephant14m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m430\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/t5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m, in \u001b[0;36mAddFadingImage\u001b[1;34m(inVideo, inImg, time, outFolder, outVideo)\u001b[0m\n\u001b[0;32m      8\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;28mmin\u001b[39m((t \u001b[38;5;241m-\u001b[39m time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;66;03m#Fading time (the difference between the visible time and the total time bounded to [0.01, 3] seconds)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m ffmpeg\u001b[38;5;241m.\u001b[39minput(inImg, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\\\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfade\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m, st\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, d\u001b[38;5;241m=\u001b[39mduration, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\\\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfade\u001b[39m\u001b[38;5;124m\"\u001b[39m, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m, st\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(duration\u001b[38;5;241m+\u001b[39mtime, t \u001b[38;5;241m-\u001b[39m duration), d\u001b[38;5;241m=\u001b[39mduration, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#The start time (st) is set to the fade in + duration or the total time - the fade duration if the input duration is too long\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m(\u001b[49m\u001b[43minVideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverlay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutFolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutVideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0:a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverwrite_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\virtual_envs\\env3\\lib\\site-packages\\ffmpeg\\_run.py:325\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(stream_spec, cmd, capture_stdout, capture_stderr, input, quiet, overwrite_output)\u001b[0m\n\u001b[0;32m    323\u001b[0m retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffmpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, out, err)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, err\n",
      "\u001b[1;31mError\u001b[0m: ffmpeg error (see stderr output for detail)"
     ]
    }
   ],
   "source": [
    "# Test your function here.\n",
    "#AddFadingImage(\"resources/video/sample1.mp4\", \"resources/images/elephant14m.png\", 430, \"results/t5\", \"result.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.6: Create a Video Collage - blend two videos into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here.\n",
    "\n",
    "#Better way to select layout style\n",
    "class Layout(Enum):\n",
    "    HORIZONTAL = True\n",
    "    VERTICAL = False\n",
    "# Replace the parameters and return type of the following function according to the task specification. \n",
    "def VideoClipMixer(\n",
    "        inVideo1: str,       \\\n",
    "        inVideo2: str,        \\\n",
    "        layout: Layout, \\\n",
    "        outFolder: str,         \\\n",
    "        outVideo: str) -> None: \n",
    "    \n",
    "    outputDir(outFolder)#Generate the output folder if it doesn't exist\n",
    "    \n",
    "    i = int(layout.value) #turn the Layout boolean into an int (HORIZONTAL = True = 1, VERTICAL = False = 0)\n",
    "    ni = 1-i #negated i\n",
    "    \n",
    "    p1 = ffmpeg.probe(inVideo1)[\"streams\"][0]\n",
    "    p2 = ffmpeg.probe(inVideo2)[\"streams\"][0]\n",
    "    \n",
    "    #get the video streams' dimensions\n",
    "    p1 = [int(p1[\"width\"]), int(p1[\"height\"])]\n",
    "    p2 = [int(p2[\"width\"]), int(p2[\"height\"])]\n",
    "    \n",
    "    #get the max of the two\n",
    "    dimensions = [str(max(p1[0], p2[0])), str(max(p1[1], p2[1]))]\n",
    "    \n",
    "    dimensions[ni] = \"temp\" #only keep the shared axis (width in VERTICAL, height in HORIZONTAL)\n",
    "    \n",
    "    #Turn the shared dimension into a proportional scale\n",
    "    p1[ni] *= int(dimensions[i]) / p1[i]\n",
    "    p2[ni] *= int(dimensions[i]) / p2[i]\n",
    "    \n",
    "    #scale the videos in the shared axis\n",
    "    in0 = ffmpeg.input(inVideo1).filter(\"scale\",\"x\".join(dimensions).replace(\"temp\", str(int(p1[ni]))))\n",
    "    in1 = ffmpeg.input(inVideo2).filter(\"scale\",\"x\".join(dimensions).replace(\"temp\", str(int(p1[ni]))))\n",
    "    \n",
    "    #join the videos in the selected direction, vstack if VERTICAL, hstack if HORIZONTAL\n",
    "    out = ffmpeg.filter((in0, in1), [\"vstack\", \"hstack\"][i])\n",
    "    #                                                 FPS (if not set, infinite frames are generated)\n",
    "    ffmpeg.output(out, outFolder+\"/\"+outVideo+\".mp4\", r=60)\\\n",
    "        .run(overwrite_output=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function here.\n",
    "#VideoClipMixer(\"resources/video/sample2.mp4\", \"resources/video/sample3.avi\", Layout.VERTICAL, \"results/t6\", \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
